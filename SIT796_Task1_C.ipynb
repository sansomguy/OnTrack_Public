{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIT796 - Task1_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPWltJ4mpq9uLBTrDgVr1uk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sansomguy/OnTrack_Public/blob/main/SIT796_Task1_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEyc_9Bp4p-8"
      },
      "source": [
        "# Task 1.C\r\n",
        "## SIT796 - Joshua Sansom-Sherwill - 220013964\r\n",
        "---\r\n",
        "## 1. Environment\r\n",
        "For our environment we are going to be creating a Traffic Light T-Intersection.\r\n",
        "The traffic light environment has been defined in task 1.P, however I will give a brief summary of what it's doing.\r\n",
        "\r\n",
        "* A negative reward is given for \"traffic\" waiting at the intersection.\r\n",
        "* A positive reward is given for traffic flowing through the intersection.\r\n",
        "* The longer the traffic is waiting at an intersection the less the reward. \r\n",
        "* The intersection is made up of 3 \"phases\". Each of these phases represents directions in which traffic can travel (think of a T-intersection).\r\n",
        "* A traffic control policy can choose from only 4 actions: 1 for each phase, and 1 for doing nothing.\r\n",
        "\r\n",
        "The code below is an attempt to define this environment for the gym framework.\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5psOc4k-TLg"
      },
      "source": [
        "import gym\r\n",
        "from gym import spaces\r\n",
        "import numpy as np\r\n",
        "import random as rand\r\n",
        "\r\n",
        "class TrafficControlEnv(gym.Env):\r\n",
        "  # create properties required for representing \r\n",
        "  # and update state of our traffic intersection\r\n",
        "  def __init__(self, u, r):\r\n",
        "    super(TrafficControlEnv, self).__init__()\r\n",
        "    # action & observation spaces\r\n",
        "    self.action_space = spaces.Discrete(4)\r\n",
        "    self.observation_space = spaces.Discrete(9)\r\n",
        "    self.u = u # phase transition time\r\n",
        "    self.r = r # waiting transition time\r\n",
        "    self.active_phase = np.array([0,0,0])\r\n",
        "    self.transition_time = 0\r\n",
        "    self.waiting_time = np.array([0,0,0])\r\n",
        "    self.flow = np.array([0,0,0])\r\n",
        "    rand.seed(None)\r\n",
        "\r\n",
        "  # clean state\r\n",
        "  def reset(self):\r\n",
        "    # All traffic is stopped at start\r\n",
        "    self.phase = np.array([0,0,0]) \r\n",
        "    # reset to some initial traffic\r\n",
        "    self.traffic = np.array([0,1,0]) \r\n",
        "    # no traffic flow yet\r\n",
        "    self.flow = np.array([0,0,0]) \r\n",
        "    # total time transitioning from one phase to another\r\n",
        "    self.transition_time = 0 \r\n",
        "    # each phase's respective time where traffic has spent waiting\r\n",
        "    self.waiting_time = np.array([0,0,0]) \r\n",
        "\r\n",
        "    return self._observe_traffic()\r\n",
        "\r\n",
        "  # step through time using new action\r\n",
        "  def step(self, action):\r\n",
        "    previous_phase = get_index(self.phase, 1)\r\n",
        "    self._perform(action)\r\n",
        "\r\n",
        "    # calculate waiting\r\n",
        "    # need to consider whether a phase transition is taking place\r\n",
        "    if self.transition_time < self.u:\r\n",
        "      self.transition_time += 1\r\n",
        "    else:\r\n",
        "      self.active_phase = self.phase\r\n",
        "  \r\n",
        "    # update our traffic waiting time \r\n",
        "    # based on phase, action, and transition times\r\n",
        "    self._update_waiting_time()\r\n",
        "    \r\n",
        "    # add random traffic\r\n",
        "    self._add_traffic()\r\n",
        "\r\n",
        "    # randomly define flow of traffic\r\n",
        "    self._update_flow()\r\n",
        "\r\n",
        "    reward = self._get_reward()\r\n",
        "    \r\n",
        "    # get our obs array\r\n",
        "    obs = self._observe_traffic()\r\n",
        "\r\n",
        "    # then we know we have more than one phase transition occurring\r\n",
        "    # which is not allowed in our traffic lights because it can cause an accident\r\n",
        "    done = previous_phase != get_index(self.phase, 1) and previous_phase != get_index(self.active_phase, 1)\r\n",
        "\r\n",
        "    return obs, reward, done, {}\r\n",
        "  \r\n",
        "  # calculate the reward of current state\r\n",
        "  def _get_reward(self):\r\n",
        "    # get the category of highest waiting time\r\n",
        "    # None, Short, Medium, Long\r\n",
        "    max_wait = min(max(self.waiting_time), 3)\r\n",
        "    wait_category = int((max_wait - (max_wait % self.r))/self.r)\r\n",
        "    flow_category = get_index(self.flow,1) + 1\r\n",
        "    # create our reward lookup table\r\n",
        "    reward_table = [[0,-2,-3, -4],\r\n",
        "                    [1,-1,-2,-3],\r\n",
        "                    [2,0,-1,-2],\r\n",
        "                    [3,1,0,-1]]\r\n",
        "    return reward_table[flow_category][wait_category];\r\n",
        "    \r\n",
        "\r\n",
        "  # randomly define and add some traffic\r\n",
        "  # under a certain probability constraint\r\n",
        "  # to simulate gradual updates of traffic\r\n",
        "  def _add_traffic(self):\r\n",
        "    # randomly add some traffic\r\n",
        "    add_traffic = rand.randint(0,10) >= 9 # add traffic 10% of the time\r\n",
        "    if add_traffic:\r\n",
        "      new_traffic = rand.randint(0,2) \r\n",
        "      self.traffic[new_traffic] = 1\r\n",
        "\r\n",
        "  # randomly update the traffic flow of the intersection\r\n",
        "  def _update_flow(self):\r\n",
        "    self.flow = np.array([0,0,0])\r\n",
        "    self.flow[rand.randint(0,2)] = 1\r\n",
        "\r\n",
        "  # return discrete observation from traffic intersection model\r\n",
        "  def _observe_traffic(self):\r\n",
        "    return np.append(np.append(self.phase, self.traffic), self.flow)\r\n",
        "\r\n",
        "  def _perform(self, action):\r\n",
        "    assert self.action_space.contains(action)\r\n",
        "    previous_phase = self.phase\r\n",
        "\r\n",
        "    if action == 0: # noop\r\n",
        "      self.phase = self.phase\r\n",
        "    elif action == 1: # phase 1 # Allow middle/ceter lane to flow\r\n",
        "      self.phase = np.array([1,0,0])\r\n",
        "    elif action == 2: # phase 2 # Allow left lane to flow\r\n",
        "      self.phase = np.array([0,1,0])\r\n",
        "    elif action == 3: # phase 3 # Allow right lane to flow\r\n",
        "      self.phase = np.array([0,0,1])\r\n",
        "\r\n",
        "    # account for transition time\r\n",
        "    if get_index(previous_phase, 1) != get_index(self.phase, 1):\r\n",
        "      self.transition_time = 0\r\n",
        "\r\n",
        "  # calculate a discrete value of waiting time based on current traffic\r\n",
        "  def _update_waiting_time(self):\r\n",
        "    # Calculate the waiting times considering active phase\r\n",
        "    self.traffic = self.traffic - self.active_phase\r\n",
        "\r\n",
        "    # increment items that are still waiting\r\n",
        "    self.waiting_time += self.traffic \r\n",
        "    # remove items that are no longer waiting\r\n",
        "    no_traffic_indexes = [i for i in range(len(self.traffic)) if self.traffic[i] == 0 or self.traffic[i] < 0]\r\n",
        "    for i in no_traffic_indexes:\r\n",
        "      self.waiting_time[i] = 0 \r\n",
        "      self.traffic[i] = 0\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# return index of item in 1D array\r\n",
        "# return -1 if not find\r\n",
        "def get_index(arr, val):\r\n",
        "  indexes = np.where(arr == val)\r\n",
        "  if len(indexes[0]) > 0:\r\n",
        "    return indexes[0][0]\r\n",
        "  else:\r\n",
        "    return -1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKGMII3D8VfU"
      },
      "source": [
        "# 2. Policy\r\n",
        "Here I create a policy to managae the traffic lights environment we have created above.\r\n",
        "\r\n",
        "This policy is not going to be creating a statistical model to help it choose actions based on previous rewards, but rather just a rule based model.\r\n",
        "\r\n",
        "Here are the rules this policy is going to implement in order to avoid accidents and maximize traffic flow.\r\n",
        "\r\n",
        "1. Given traffic, begin phase transition with respect to that traffic.\r\n",
        "2. Each phase has a minimum time to run of 4 time steps.\r\n",
        "3. We will always initiate the phase for which we spotted traffic the earliest.\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdCWABDh8a2a"
      },
      "source": [
        "# This class is going to manage our policy actions\r\n",
        "# some state like last known traffic is important to remember between time steps\r\n",
        "class PolicyModel:\r\n",
        "  def __init__(self):\r\n",
        "    self.min_phase = 4 # minimum time steps for one phase of traffic\r\n",
        "    self.first_traffic = -1 # keep track of the current traffic information\r\n",
        "\r\n",
        "  def policy(self, obs, t):\r\n",
        "    #test whether we should make a change at this point in time game\r\n",
        "    if self.first_traffic == -1:\r\n",
        "      traffic = obs[3:6] # find the traffic information from our discrete observation\r\n",
        "      self.first_traffic = get_index(traffic, 1) # find the index of any active traffic\r\n",
        "\r\n",
        "    # if we are still t < min_phase time steps into our phase then we know we can't do anything yet\r\n",
        "    # or if we have no traffic, we know we also don't need to do anything yet\r\n",
        "    if t % self.min_phase != 0 or self.first_traffic == -1:\r\n",
        "      return 0 # do nothing action\r\n",
        "\r\n",
        "    action = self.first_traffic + 1\r\n",
        "    # reset traffic to the next\r\n",
        "    self.first_traffic = -1\r\n",
        "\r\n",
        "    return action\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrXvQ5cVABxw"
      },
      "source": [
        "# 3. Execute & Test\r\n",
        "Here we setup our traffic environment using the standard Gym environment boilerplate code. However, we are going to be utilising our very own policy model to choose which actions to take next.\r\n",
        "\r\n",
        "Importantly, there is no winning state for this game, the traffic control policies just have to try and earn as much reward as possible over the time limit defined.\r\n",
        "\r\n",
        "---\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRe19IVDBokY",
        "outputId": "3d36a4d9-06a0-4724-fec9-fed34de0f7b3"
      },
      "source": [
        "TIME_LIMIT = 100\r\n",
        "\r\n",
        "# Create new traffic control gym environment\r\n",
        "env = TrafficControlEnv(3, 2)\r\n",
        "# create policy instance\r\n",
        "model = PolicyModel()\r\n",
        "\r\n",
        "# Reset to clear state from potentialy earlier runs\r\n",
        "o = env.reset() \r\n",
        "# for each time step\r\n",
        "for t in range(TIME_LIMIT):\r\n",
        "    # choose action from policy\r\n",
        "    action = model.policy(o,t)\r\n",
        "    # take step given action\r\n",
        "    o, r, d, _ = env.step(action) \r\n",
        "    #reward is not utilized by our policy because \r\n",
        "    #we are only following some basic business rules.\r\n",
        "\r\n",
        "    # print reward for each given action so we can see\r\n",
        "    # what our policy is doing to the environment\r\n",
        "    print(\"Reward {0}, Action {1}\".format(r, action))\r\n",
        "\r\n",
        "    if d and t<TIME_LIMIT-1:\r\n",
        "        print(\"Task failed in \", t, \" time steps\")\r\n",
        "        break\r\n",
        "else:\r\n",
        "    print(\"Time limit reached.\")\r\n",
        "\r\n",
        "env.reset()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward 3, Action 2\n",
            "Reward 0, Action 0\n",
            "Reward -1, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 1, Action 2\n",
            "Reward 1, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward -1, Action 0\n",
            "Reward 1, Action 1\n",
            "Reward 1, Action 0\n",
            "Reward -1, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 3, Action 1\n",
            "Reward -1, Action 0\n",
            "Reward -1, Action 0\n",
            "Reward 0, Action 0\n",
            "Reward 0, Action 3\n",
            "Reward -1, Action 0\n",
            "Reward -1, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward -1, Action 3\n",
            "Reward 1, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward -1, Action 0\n",
            "Reward -1, Action 2\n",
            "Reward 1, Action 0\n",
            "Reward -1, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 1, Action 2\n",
            "Reward 2, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 2, Action 2\n",
            "Reward 3, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 1, Action 2\n",
            "Reward 2, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward -1, Action 1\n",
            "Reward -1, Action 0\n",
            "Reward 0, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 2, Action 1\n",
            "Reward 1, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 0, Action 0\n",
            "Reward -1, Action 2\n",
            "Reward 0, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward -1, Action 2\n",
            "Reward 0, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward -1, Action 3\n",
            "Reward 0, Action 0\n",
            "Reward -1, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 3, Action 3\n",
            "Reward 1, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 1, Action 2\n",
            "Reward -1, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 3, Action 2\n",
            "Reward 2, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 3, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward -1, Action 3\n",
            "Reward 1, Action 0\n",
            "Reward 0, Action 0\n",
            "Reward 1, Action 0\n",
            "Reward 3, Action 3\n",
            "Reward 2, Action 0\n",
            "Reward 2, Action 0\n",
            "Reward 2, Action 0\n",
            "Time limit reached.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXsfhQwKTJG8"
      },
      "source": [
        "# 4. Summary\r\n",
        "From above we can see that our policy does a decent job of choosing actions that get it the greatest rewards. However this was done using some rules based on knowledge of the environment and it's constraints. In the future I would like to implement a RL based approach to optimizing these decisions, taking into account the rewards and building it's own internal value function rather than having to devise a policy function based on the mechanics of the game."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXx7cUd3T5wJ"
      },
      "source": [
        "# 5. Reference List\r\n",
        "1. Adam King, \"Create custom gym environments from scratch — A stock market example\", Towards Datascience,  https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e (Accessed March 13, 2021)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-LEuDKYUdE0"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}